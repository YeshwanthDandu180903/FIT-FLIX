{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a77cd06-e6fa-47e3-9fd4-45fee8fcf879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vectorstore from 'fitflix_chroma_db_gemini'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_12868\\2368419301.py:53: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=db_persist_dir, embedding_function=gemini_embeddings)\n",
      "C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_12868\\2368419301.py:74: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "2025-08-02 06:42:26.202 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.256 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\dandu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-02 06:42:26.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.272 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.272 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.274 Session state does not function when running a script without `streamlit run`\n",
      "2025-08-02 06:42:26.274 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-02 06:42:26.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# app.py (This is what you would save and run with `streamlit run app.py`)\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import streamlit as st # Import Streamlit\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import shutil\n",
    "\n",
    "# --- Setup for RAG Chain (all this should be executed once when the app starts) ---\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize LLM\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.7) # Use your confirmed model name\n",
    "\n",
    "# Data Loading and Chunking\n",
    "knowledge_base_path = \"knowledge_base\"\n",
    "markdown_file_glob = \"**/*.md\"\n",
    "loader = DirectoryLoader(\n",
    "    knowledge_base_path, glob=markdown_file_glob, loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "headers_to_split_on = [(\"#\", \"Header1\"), (\"##\", \"Header2\")]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    split_docs = markdown_splitter.split_text(doc.page_content)\n",
    "    for s_doc in split_docs:\n",
    "        s_doc.metadata = {**doc.metadata, **s_doc.metadata}\n",
    "    chunks.extend(split_docs)\n",
    "\n",
    "# Initialize Embeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create/Load Vector Store\n",
    "db_persist_dir = \"fitflix_chroma_db_gemini\"\n",
    "# Only create/load if it doesn't exist to save time on reruns\n",
    "if not os.path.exists(db_persist_dir) or not os.listdir(db_persist_dir): # Check if directory is empty or doesn't exist\n",
    "    print(f\"Creating new vectorstore at '{db_persist_dir}'...\")\n",
    "    vectorstore = Chroma.from_documents(documents=chunks, embedding=gemini_embeddings, persist_directory=db_persist_dir)\n",
    "    vectorstore.persist()\n",
    "else:\n",
    "    print(f\"Loading existing vectorstore from '{db_persist_dir}'...\")\n",
    "    vectorstore = Chroma(persist_directory=db_persist_dir, embedding_function=gemini_embeddings)\n",
    "\n",
    "# Create Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define Prompt Template\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in information about Fitflix entities.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer concise, professional, and directly address the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Set up Conversational Memory (this is what LangChain's chain uses internally)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create the Conversational Retrieval QA Chain\n",
    "# This is your core RAG engine\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=gemini_llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory, # This memory object is key\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# --- Streamlit UI and Chat Logic ---\n",
    "\n",
    "st.title(\"Fitflix RAG Chatbot\")\n",
    "st.write(\"Ask me questions about Fitflix from the knowledge base.\")\n",
    "\n",
    "# Initialize chat history in Streamlit's session state\n",
    "# This is for DISPLAYING the conversation in the UI\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Accept user input\n",
    "if prompt := st.chat_input(\"Ask your question here...\"):\n",
    "    # Add user message to chat history (for display)\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Get assistant response from the RAG chain\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            # Call the qa_chain's invoke method\n",
    "            # The qa_chain uses its internal 'memory' object to maintain context\n",
    "            result = qa_chain.invoke({\"question\": prompt})\n",
    "            full_response = result[\"answer\"]\n",
    "            st.markdown(full_response)\n",
    "    # Add assistant response to chat history (for display)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "# Optional: Clear chat history button\n",
    "if st.button(\"Clear Chat\"):\n",
    "    st.session_state.messages = []\n",
    "    # Also clear LangChain's internal memory\n",
    "    memory.clear()\n",
    "    st.rerun() # Use st.rerun() instead of st.experimental_rerun() for newer Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927698c5-dd63-48ac-9deb-f4517e490622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
