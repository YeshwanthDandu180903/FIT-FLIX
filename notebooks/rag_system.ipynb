{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178261ff-c835-4a1c-be89-31fde2481522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini LLM initialized and working.\n"
     ]
    }
   ],
   "source": [
    "# rag.ipynb\n",
    "\n",
    "# --- IMPORTS ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import shutil\n",
    "import gradio as gr\n",
    "\n",
    "# --- LOAD ENV VARIABLES ---\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "\n",
    "# --- Initialize Gemini LLM ---\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.3)\n",
    "\n",
    "try:\n",
    "    _ = gemini_llm.invoke(\"Test LLM connection.\")\n",
    "    print(\"‚úÖ Gemini LLM initialized and working.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Gemini LLM: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- SETTINGS ---\n",
    "DOCS_PATH = \"knowledge_base\"  # Folder with Markdown/Text files\n",
    "CHROMA_DB_PATH = \"chroma_db_fitflix\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d26d294-be85-4bc0-83b0-4c82d5afe870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 9 documents.\n",
      "‚úÖ Split into 49 text chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Load Documents ---\n",
    "loader = DirectoryLoader(DOCS_PATH, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents.\")\n",
    "\n",
    "# --- STEP 2: Split Text into Chunks ---\n",
    "text_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    ")\n",
    "docs = []\n",
    "for doc in documents:\n",
    "    docs.extend(text_splitter.split_text(doc.page_content))\n",
    "\n",
    "print(f\"‚úÖ Split into {len(docs)} text chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39dbd347-ee5e-416c-a9e1-d73f47fcf276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chroma Vector Store created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_26456\\3558046920.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 3: Initialize Embeddings and Chroma Vector Store ---\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Remove old DB if exists\n",
    "if os.path.exists(CHROMA_DB_PATH):\n",
    "    shutil.rmtree(CHROMA_DB_PATH)\n",
    "\n",
    "vectorstore = Chroma.from_texts([d.page_content for d in docs], embedding=embeddings, persist_directory=CHROMA_DB_PATH)\n",
    "vectorstore.persist()\n",
    "print(\"‚úÖ Chroma Vector Store created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb1bb9d-8f44-40f0-9f3b-ad892cf21731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- STEP 4: Setup RAG Pipeline ---\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Minimal response template for concise answers\n",
    "prompt_template = \"\"\"You are an AI assistant specialized in information about Fitflix entities.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "and try to interactive if any body greets you greet them also. and follow the humanity rules\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer concise, professional, and directly address the question.\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key=\"answer\")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=gemini_llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb39c798-b711-49c7-bdce-8feacdd67aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_26456\\3466950162.py:9: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5: Create Gradio Chat Interface ---\n",
    "def chat_fn(message, history):\n",
    "    result = rag_chain.invoke({\"question\": message})\n",
    "    answer = result[\"answer\"]\n",
    "    return answer\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üèãÔ∏è Fitflix RAG Chatbot (Powered by Gemini)\")\n",
    "    chatbot = gr.Chatbot(height=400)\n",
    "    msg = gr.Textbox(label=\"Ask me about fitness, workouts, or diet...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        response = chat_fn(message, chat_history)\n",
    "        chat_history.append((message, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9616c3-d6a9-484b-8ac4-8e49b07b62ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
