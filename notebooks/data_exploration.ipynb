{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740bafc8",
   "metadata": {},
   "source": [
    "# FIT-FLIX Knowledge Base Data Exploration\n",
    "\n",
    "This notebook explores and analyzes the fitness knowledge base for the FIT-FLIX RAG system.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Overview\n",
    "2. Document Statistics\n",
    "3. Content Analysis\n",
    "4. Category Distribution\n",
    "5. Text Processing Insights\n",
    "6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from src.config import Config\n",
    "from src.utils.document_loader import DocumentLoader\n",
    "from src.utils.text_splitter import TextSplitter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07122c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "config = Config()\n",
    "document_loader = DocumentLoader(config)\n",
    "text_splitter = TextSplitter(config)\n",
    "\n",
    "print(f\"Knowledge base directory: {config.knowledge_base_dir}\")\n",
    "print(f\"Chunk size: {config.chunk_size}\")\n",
    "print(f\"Chunk overlap: {config.chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4359fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents\n",
    "documents = document_loader.load_all_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Get document statistics\n",
    "stats = document_loader.get_document_stats(documents)\n",
    "print(\"\\nDocument Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "doc_data = []\n",
    "for doc in documents:\n",
    "    metadata = doc['metadata']\n",
    "    content = doc['content']\n",
    "    \n",
    "    doc_data.append({\n",
    "        'source': metadata.get('source', 'unknown'),\n",
    "        'category': metadata.get('category', 'unknown'),\n",
    "        'file_type': metadata.get('file_type', 'unknown'),\n",
    "        'content_length': len(content),\n",
    "        'word_count': len(content.split()),\n",
    "        'line_count': content.count('\\n') + 1,\n",
    "        'content': content\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(doc_data)\n",
    "print(\"Document DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5005eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document length distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(df['content_length'], bins=20, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Content Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(df['word_count'], bins=20, alpha=0.7, color='lightgreen')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Counts')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "category_counts = df['category'].value_counts()\n",
    "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution by Category')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df, x='category', y='content_length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Content Length by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text splitting\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "split_stats = text_splitter.get_splitting_stats(chunked_documents)\n",
    "\n",
    "print(\"Text Splitting Statistics:\")\n",
    "for key, value in split_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create DataFrame for chunks\n",
    "chunk_data = []\n",
    "for chunk in chunked_documents:\n",
    "    metadata = chunk['metadata']\n",
    "    content = chunk['content']\n",
    "    \n",
    "    chunk_data.append({\n",
    "        'source': metadata.get('source', 'unknown'),\n",
    "        'category': metadata.get('category', 'unknown'),\n",
    "        'chunk_id': metadata.get('chunk_id', 0),\n",
    "        'total_chunks': metadata.get('total_chunks', 1),\n",
    "        'content_length': len(content),\n",
    "        'word_count': len(content.split())\n",
    "    })\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "print(f\"\\nCreated {len(chunk_df)} chunks from {len(df)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(chunk_df['content_length'], bins=30, alpha=0.7, color='coral')\n",
    "plt.axvline(config.chunk_size, color='red', linestyle='--', label=f'Target Size ({config.chunk_size})')\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "chunks_per_doc = chunk_df.groupby('source')['chunk_id'].max() + 1\n",
    "plt.hist(chunks_per_doc, bins=15, alpha=0.7, color='gold')\n",
    "plt.xlabel('Number of Chunks per Document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Chunks per Document Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content analysis - most common words\n",
    "all_text = ' '.join(df['content'].tolist())\n",
    "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Remove common stop words\n",
    "stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "filtered_words = {word: count for word, count in word_freq.items() if word not in stop_words and len(word) > 2}\n",
    "top_words = dict(Counter(filtered_words).most_common(20))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_words.keys(), top_words.values(), color='lightblue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 20 Most Common Words in Knowledge Base')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 words:\")\n",
    "for word, count in list(top_words.items())[:10]:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== FIT-FLIX Knowledge Base Summary ===\")\n",
    "print(f\"Total Documents: {len(documents)}\")\n",
    "print(f\"Total Chunks: {len(chunked_documents)}\")\n",
    "print(f\"Categories: {list(df['category'].unique())}\")\n",
    "print(f\"Total Words: {df['word_count'].sum():,}\")\n",
    "print(f\"Total Characters: {df['content_length'].sum():,}\")\n",
    "print(f\"Average Document Length: {df['content_length'].mean():.0f} characters\")\n",
    "print(f\"Average Chunk Length: {chunk_df['content_length'].mean():.0f} characters\")\n",
    "print(f\"Chunk Size Efficiency: {(chunk_df['content_length'].mean() / config.chunk_size) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n=== Category Breakdown ===\")\n",
    "category_stats = df.groupby('category').agg({\n",
    "    'content_length': ['count', 'sum', 'mean'],\n",
    "    'word_count': 'sum'\n",
    "}).round(2)\n",
    "print(category_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
