{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570afad0",
   "metadata": {},
   "source": [
    "# FIT-FLIX RAG System Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the FIT-FLIX RAG system.\n",
    "\n",
    "## Contents\n",
    "1. System Setup and Initialization\n",
    "2. Test Questions and Ground Truth\n",
    "3. Retrieval Evaluation\n",
    "4. Generation Quality Assessment\n",
    "5. End-to-End Performance\n",
    "6. Performance Metrics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from src.config import Config\n",
    "from src.retrieval.retriever import DocumentRetriever\n",
    "from src.generation.llm_manager import LLMManager\n",
    "from src.utils.document_loader import DocumentLoader\n",
    "from src.embeddings.embedding_manager import EmbeddingManager\n",
    "from src.utils.text_splitter import TextSplitter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG system\n",
    "config = Config()\n",
    "document_loader = DocumentLoader(config)\n",
    "text_splitter = TextSplitter(config)\n",
    "retriever = DocumentRetriever(config)\n",
    "llm_manager = LLMManager(config)\n",
    "\n",
    "print(\"ðŸš€ Initializing FIT-FLIX RAG System for evaluation...\")\n",
    "\n",
    "# Load and process documents if needed\n",
    "try:\n",
    "    retriever.initialize()\n",
    "    collection_info = retriever.get_retrieval_stats()\n",
    "    print(f\"âœ… Vector store loaded with {collection_info.get('document_count', 0)} documents\")\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ“š Loading documents into vector store...\")\n",
    "    documents = document_loader.load_all_documents()\n",
    "    chunked_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Add documents to retriever\n",
    "    retriever.initialize()\n",
    "    doc_contents = [doc['content'] for doc in chunked_docs]\n",
    "    doc_metadata = [doc['metadata'] for doc in chunked_docs]\n",
    "    retriever.add_documents(doc_contents, doc_metadata)\n",
    "    print(f\"âœ… Added {len(chunked_docs)} document chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a61ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions with expected categories\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"What types of fitness classes do you offer?\",\n",
    "        \"expected_category\": \"classes\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much does a monthly membership cost?\",\n",
    "        \"expected_category\": \"membership\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the gym's operating hours?\",\n",
    "        \"expected_category\": \"about\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you recommend a good post-workout nutrition plan?\",\n",
    "        \"expected_category\": \"nutrition\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What qualifications do your personal trainers have?\",\n",
    "        \"expected_category\": \"trainers\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can I cancel my membership?\",\n",
    "        \"expected_category\": \"membership\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Do you have specialized equipment for powerlifting?\",\n",
    "        \"expected_category\": \"facilities\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should I eat before a high-intensity workout?\",\n",
    "        \"expected_category\": \"nutrition\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“ Created {len(test_questions)} test questions\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. [{q['difficulty'].upper()}] {q['question']} (Expected: {q['expected_category']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval performance\n",
    "def evaluate_retrieval(questions: List[Dict], retriever: DocumentRetriever, top_k: int = 5):\n",
    "    \"\"\"Evaluate retrieval performance.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q_data in questions:\n",
    "        question = q_data['question']\n",
    "        expected_category = q_data['expected_category']\n",
    "        difficulty = q_data['difficulty']\n",
    "        \n",
    "        start_time = time.time()\n",
    "        retrieved_docs = retriever.retrieve(question, n_results=top_k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Check if expected category is in retrieved docs\n",
    "        retrieved_categories = [doc['metadata'].get('category', 'unknown') for doc in retrieved_docs]\n",
    "        category_match = expected_category in retrieved_categories\n",
    "        \n",
    "        # Calculate average similarity\n",
    "        avg_similarity = np.mean([doc['similarity'] for doc in retrieved_docs]) if retrieved_docs else 0\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'expected_category': expected_category,\n",
    "            'difficulty': difficulty,\n",
    "            'retrieved_count': len(retrieved_docs),\n",
    "            'category_match': category_match,\n",
    "            'avg_similarity': avg_similarity,\n",
    "            'top_similarity': retrieved_docs[0]['similarity'] if retrieved_docs else 0,\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'retrieved_categories': retrieved_categories[:3]  # Top 3 categories\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"ðŸ” Evaluating retrieval performance...\")\n",
    "retrieval_results = evaluate_retrieval(test_questions, retriever)\n",
    "retrieval_df = pd.DataFrame(retrieval_results)\n",
    "\n",
    "print(\"\\nðŸ“Š Retrieval Results Summary:\")\n",
    "print(f\"Category Match Rate: {retrieval_df['category_match'].mean():.2%}\")\n",
    "print(f\"Average Similarity Score: {retrieval_df['avg_similarity'].mean():.3f}\")\n",
    "print(f\"Average Retrieval Time: {retrieval_df['retrieval_time'].mean():.3f}s\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for _, row in retrieval_df.iterrows():\n",
    "    match_status = \"âœ…\" if row['category_match'] else \"âŒ\"\n",
    "    print(f\"{match_status} {row['question'][:50]}... | Sim: {row['top_similarity']:.3f} | Time: {row['retrieval_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate generation performance\n",
    "def evaluate_generation(questions: List[Dict], retriever: DocumentRetriever, llm_manager: LLMManager):\n",
    "    \"\"\"Evaluate generation performance.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q_data in questions:\n",
    "        question = q_data['question']\n",
    "        difficulty = q_data['difficulty']\n",
    "        \n",
    "        try:\n",
    "            # Retrieve context\n",
    "            retrieved_docs = retriever.retrieve(question)\n",
    "            \n",
    "            # Generate response\n",
    "            start_time = time.time()\n",
    "            response = llm_manager.generate_response(question, retrieved_docs)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            # Basic quality metrics\n",
    "            response_length = len(response)\n",
    "            word_count = len(response.split())\n",
    "            has_context = len(retrieved_docs) > 0\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'difficulty': difficulty,\n",
    "                'response': response,\n",
    "                'response_length': response_length,\n",
    "                'word_count': word_count,\n",
    "                'generation_time': generation_time,\n",
    "                'has_context': has_context,\n",
    "                'context_docs': len(retrieved_docs),\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'difficulty': difficulty,\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'response_length': 0,\n",
    "                'word_count': 0,\n",
    "                'generation_time': 0,\n",
    "                'has_context': False,\n",
    "                'context_docs': 0,\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Note: This will only work if you have valid API keys configured\n",
    "print(\"ðŸ¤– Evaluating generation performance...\")\n",
    "print(\"Note: This requires valid API keys in your .env file\")\n",
    "\n",
    "try:\n",
    "    generation_results = evaluate_generation(test_questions[:3], retriever, llm_manager)  # Test first 3 questions\n",
    "    generation_df = pd.DataFrame(generation_results)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Generation Results Summary:\")\n",
    "    print(f\"Success Rate: {generation_df['success'].mean():.2%}\")\n",
    "    print(f\"Average Response Length: {generation_df['response_length'].mean():.0f} characters\")\n",
    "    print(f\"Average Word Count: {generation_df['word_count'].mean():.0f} words\")\n",
    "    print(f\"Average Generation Time: {generation_df['generation_time'].mean():.3f}s\")\n",
    "    \n",
    "    print(\"\\nSample Responses:\")\n",
    "    for _, row in generation_df.iterrows():\n",
    "        if row['success']:\n",
    "            print(f\"\\nQ: {row['question']}\")\n",
    "            print(f\"A: {row['response'][:200]}...\" if len(row['response']) > 200 else f\"A: {row['response']}\")\n",
    "        else:\n",
    "            print(f\"\\nQ: {row['question']}\")\n",
    "            print(f\"âŒ Failed: {row['response']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Generation evaluation skipped: {str(e)}\")\n",
    "    print(\"Make sure you have valid API keys configured in your .env file\")\n",
    "    generation_df = pd.DataFrame()  # Empty dataframe for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Retrieval performance by difficulty\n",
    "if not retrieval_df.empty:\n",
    "    difficulty_performance = retrieval_df.groupby('difficulty')['category_match'].mean()\n",
    "    axes[0, 0].bar(difficulty_performance.index, difficulty_performance.values, color=['green', 'orange', 'red'])\n",
    "    axes[0, 0].set_title('Retrieval Accuracy by Difficulty')\n",
    "    axes[0, 0].set_ylabel('Category Match Rate')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Similarity score distribution\n",
    "if not retrieval_df.empty:\n",
    "    axes[0, 1].hist(retrieval_df['avg_similarity'], bins=10, alpha=0.7, color='skyblue')\n",
    "    axes[0, 1].set_title('Distribution of Similarity Scores')\n",
    "    axes[0, 1].set_xlabel('Average Similarity Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Performance timing\n",
    "if not retrieval_df.empty:\n",
    "    axes[1, 0].scatter(retrieval_df['retrieval_time'], retrieval_df['avg_similarity'], \n",
    "                      c=['green' if match else 'red' for match in retrieval_df['category_match']], alpha=0.7)\n",
    "    axes[1, 0].set_title('Retrieval Time vs Similarity')\n",
    "    axes[1, 0].set_xlabel('Retrieval Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Average Similarity')\n",
    "\n",
    "# Generation performance (if available)\n",
    "if not generation_df.empty and len(generation_df) > 0:\n",
    "    axes[1, 1].scatter(generation_df['generation_time'], generation_df['word_count'], \n",
    "                      c=['green' if success else 'red' for success in generation_df['success']], alpha=0.7)\n",
    "    axes[1, 1].set_title('Generation Time vs Response Length')\n",
    "    axes[1, 1].set_xlabel('Generation Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Word Count')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Generation evaluation\\nnot available\\n(API keys required)', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 1].set_title('Generation Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3538fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary and recommendations\n",
    "print(\"=== FIT-FLIX RAG SYSTEM EVALUATION SUMMARY ===\")\n",
    "print(f\"\\nðŸ“Š RETRIEVAL PERFORMANCE:\")\n",
    "print(f\"â€¢ Category Match Rate: {retrieval_df['category_match'].mean():.1%}\")\n",
    "print(f\"â€¢ Average Similarity Score: {retrieval_df['avg_similarity'].mean():.3f}\")\n",
    "print(f\"â€¢ Average Retrieval Time: {retrieval_df['retrieval_time'].mean():.3f} seconds\")\n",
    "print(f\"â€¢ Best performing difficulty: {retrieval_df.groupby('difficulty')['category_match'].mean().idxmax()}\")\n",
    "print(f\"â€¢ Worst performing difficulty: {retrieval_df.groupby('difficulty')['category_match'].mean().idxmin()}\")\n",
    "\n",
    "if not generation_df.empty and len(generation_df) > 0:\n",
    "    print(f\"\\nðŸ¤– GENERATION PERFORMANCE:\")\n",
    "    print(f\"â€¢ Success Rate: {generation_df['success'].mean():.1%}\")\n",
    "    print(f\"â€¢ Average Response Length: {generation_df['response_length'].mean():.0f} characters\")\n",
    "    print(f\"â€¢ Average Generation Time: {generation_df['generation_time'].mean():.3f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nðŸ¤– GENERATION PERFORMANCE: Not evaluated (requires API keys)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "if retrieval_df['category_match'].mean() < 0.8:\n",
    "    print(\"â€¢ Consider improving document chunking strategy\")\n",
    "    print(\"â€¢ Review embedding model performance\")\n",
    "    print(\"â€¢ Add more diverse training examples\")\n",
    "\n",
    "if retrieval_df['avg_similarity'].mean() < 0.7:\n",
    "    print(\"â€¢ Consider fine-tuning embedding model on domain data\")\n",
    "    print(\"â€¢ Implement hybrid search (semantic + keyword)\")\n",
    "\n",
    "if retrieval_df['retrieval_time'].mean() > 1.0:\n",
    "    print(\"â€¢ Optimize vector database indexing\")\n",
    "    print(\"â€¢ Consider reducing embedding dimensions\")\n",
    "\n",
    "print(\"â€¢ Implement user feedback collection for continuous improvement\")\n",
    "print(\"â€¢ Add more comprehensive evaluation metrics\")\n",
    "print(\"â€¢ Consider A/B testing different retrieval strategies\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
