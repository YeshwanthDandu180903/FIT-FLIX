{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449261ed-2890-48ca-a427-36783a3815f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nvidia-ai-endpoints\n",
      "  Downloading langchain_nvidia_ai_endpoints-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-nvidia-ai-endpoints) (3.11.18)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-nvidia-ai-endpoints) (1.2.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.51 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-nvidia-ai-endpoints) (0.3.72)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.20.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.4.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (4.13.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (2.11.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (3.0.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (3.10)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dandu\\anaconda3\\envs\\llms\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4,>=0.3.51->langchain-nvidia-ai-endpoints) (1.3.1)\n",
      "Downloading langchain_nvidia_ai_endpoints-0.3.13-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: langchain-nvidia-ai-endpoints\n",
      "Successfully installed langchain-nvidia-ai-endpoints-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764164af-a18b-48a6-a870-9ccfc7d950ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 9 documents.\n",
      "‚úÖ Split into 49 text chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:184: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# rag.ipynb\n",
    "\n",
    "# --- IMPORTS ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import shutil\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "# --- Initialize NVIDIA LLM ---\n",
    "nvidia_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\", temperature=0.3,nvcf_api_key=\"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\")\n",
    "\n",
    "\n",
    "\n",
    "# --- SETTINGS ---\n",
    "DOCS_PATH = \"knowledge_base\"  # Folder with your Markdown/Text files\n",
    "CHROMA_DB_PATH = \"chroma_db_fitflix_vector_2\"\n",
    "\n",
    "# --- STEP 1: Load Documents ---\n",
    "loader = DirectoryLoader(DOCS_PATH, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents.\")\n",
    "\n",
    "# --- STEP 2: Split Text into Chunks ---\n",
    "text_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    ")\n",
    "docs = []\n",
    "for doc in documents:\n",
    "    docs.extend(text_splitter.split_text(doc.page_content))\n",
    "\n",
    "print(f\"‚úÖ Split into {len(docs)} text chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fd70d57-1bde-4186-966c-9cd06c54fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:227: UserWarning: Model nvidia/lama-3_2-nemoretriever-1b-vlm-embed-v1 is unknown, check `available_models`. Inference may fail.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[404] Not Found\n{'_content': b'404 page not found\\n', '_content_consumed': True, '_next': None, 'status_code': 404, 'headers': {'Date': 'Mon, 04 Aug 2025 16:45:01 GMT', 'Content-Type': 'text/plain; charset=utf-8', 'Content-Length': '19', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Content-Type-Options': 'nosniff'}, 'raw': <urllib3.response.HTTPResponse object at 0x000001F2DA207B20>, 'url': 'https://integrate.api.nvidia.com/v1/embeddings', 'encoding': 'utf-8', 'history': [], 'reason': 'Not Found', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(microseconds=249842), 'request': <PreparedRequest [POST]>, 'connection': <requests.adapters.HTTPAdapter object at 0x000001F2DA395A10>}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(CHROMA_DB_PATH):\n\u001b[32m      6\u001b[39m     shutil.rmtree(CHROMA_DB_PATH)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHROMA_DB_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m vectorstore.persist()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Chroma Vector Store created.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\embeddings.py:175\u001b[39m, in \u001b[36mNVIDIAEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), \u001b[38;5;28mself\u001b[39m.max_batch_size):\n\u001b[32m    174\u001b[39m     batch = texts[i : i + \u001b[38;5;28mself\u001b[39m.max_batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     all_embeddings.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpassage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\embeddings.py:149\u001b[39m, in \u001b[36mNVIDIAEmbeddings._embed\u001b[39m\u001b[34m(self, texts, model_type)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dimensions:\n\u001b[32m    147\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33mdimensions\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.dimensions\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_req\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m response.raise_for_status()\n\u001b[32m    153\u001b[39m result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:482\u001b[39m, in \u001b[36m_NVIDIAClient.get_req\u001b[39m\u001b[34m(self, payload, extra_headers)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_req\u001b[39m(\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    478\u001b[39m     payload: \u001b[38;5;28mdict\u001b[39m = {},\n\u001b[32m    479\u001b[39m     extra_headers: \u001b[38;5;28mdict\u001b[39m = {},\n\u001b[32m    480\u001b[39m ) -> Response:\n\u001b[32m    481\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Post to the API.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     response, session = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait(response, session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:378\u001b[39m, in \u001b[36m_NVIDIAClient._post\u001b[39m\u001b[34m(self, invoke_url, payload, extra_headers)\u001b[39m\n\u001b[32m    374\u001b[39m session = \u001b[38;5;28mself\u001b[39m.get_session_fn()\n\u001b[32m    375\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response = session.post(\n\u001b[32m    376\u001b[39m     **\u001b[38;5;28mself\u001b[39m.__add_authorization(\u001b[38;5;28mself\u001b[39m.last_inputs)\n\u001b[32m    377\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:471\u001b[39m, in \u001b[36m_NVIDIAClient._try_raise\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    469\u001b[39m     body += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease check or regenerate your API key.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: [404] Not Found\n{'_content': b'404 page not found\\n', '_content_consumed': True, '_next': None, 'status_code': 404, 'headers': {'Date': 'Mon, 04 Aug 2025 16:45:01 GMT', 'Content-Type': 'text/plain; charset=utf-8', 'Content-Length': '19', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Content-Type-Options': 'nosniff'}, 'raw': <urllib3.response.HTTPResponse object at 0x000001F2DA207B20>, 'url': 'https://integrate.api.nvidia.com/v1/embeddings', 'encoding': 'utf-8', 'history': [], 'reason': 'Not Found', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(microseconds=249842), 'request': <PreparedRequest [POST]>, 'connection': <requests.adapters.HTTPAdapter object at 0x000001F2DA395A10>}"
     ]
    }
   ],
   "source": [
    "#--- STEP 3: Initialize Embeddings and Chroma Vector Store ---\n",
    "embeddings = NVIDIAEmbeddings(model=\"nvidia/lama-3_2-nemoretriever-1b-vlm-embed-v1\")\n",
    "\n",
    "# Remove old DB if exists\n",
    "if os.path.exists(CHROMA_DB_PATH):\n",
    "    shutil.rmtree(CHROMA_DB_PATH)\n",
    "\n",
    "vectorstore = Chroma.from_texts([d.page_content for d in docs], embedding=embeddings, persist_directory=CHROMA_DB_PATH)\n",
    "vectorstore.persist()\n",
    "print(\"‚úÖ Chroma Vector Store created.\")\n",
    "\n",
    "# --- STEP 4: Setup RAG Pipeline ---\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Minimal response template\n",
    "prompt_template = \"\"\"You are Fitflix AI, a concise fitness and nutrition assistant.\n",
    "Answer the user's question using the context below in **2 sentences maximum**.\n",
    "If you are unsure, say 'I am not sure about that.'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=nvidia_llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4672200-d5ef-4a5f-a2e2-b0ced3fe4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_25204\\482280328.py:9: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1664, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_25204\\482280328.py\", line 14, in respond\n",
      "    response = chat_fn(message, chat_history)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dandu\\AppData\\Local\\Temp\\ipykernel_25204\\482280328.py\", line 3, in chat_fn\n",
      "    result = rag_chain.invoke({\"question\": message})\n",
      "             ^^^^^^^^^\n",
      "NameError: name 'rag_chain' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5: Create Gradio Chat Interface ---\n",
    "def chat_fn(message, history):\n",
    "    result = rag_chain.invoke({\"question\": message})\n",
    "    answer = result[\"answer\"]\n",
    "    return answer\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üèãÔ∏è Fitflix RAG Chatbot (Powered by NVIDIA)\")\n",
    "    chatbot = gr.Chatbot(height=400)\n",
    "    msg = gr.Textbox(label=\"Ask me about fitness, workouts, or diet...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        response = chat_fn(message, chat_history)\n",
    "        chat_history.append((message, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f2546-5f76-499f-8379-c692b513efaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
